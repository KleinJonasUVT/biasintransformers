{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TextDataset\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import json\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Paths\n",
    "filepath = \"../out/cleaned_books_small.txt\"\n",
    "save_dir = \"./custom_tokenizer\"\n",
    "vocab_file = f\"{save_dir}/custom_vocab_small-vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a small text of a big corpus for testing the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_original = \"../out/cleaned_books.txt\"\n",
    "\n",
    "# I want to create a small version of the cleaned_books.txt file for testing\n",
    "with open(filepath_original, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"../out/cleaned_books_small.txt\", \"w\") as f:\n",
    "    f.writelines(lines[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a tokenizer on own text data\n",
    "\n",
    "This code trains a **WordPiece tokenizer** on a our own dataset and saves it for later use. Instead of tokenizing immediately, storing the trained tokenizer allows for on-the-fly tokenization. This way we can use it for different datasets and models as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer training complete! Saved to ./custom_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Initialize a WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,\n",
    ")\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(\n",
    "    files=[filepath],\n",
    "    vocab_size=5000,\n",
    "    min_frequency=3,\n",
    "    limit_alphabet=1000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_model(save_dir, \"custom_vocab_small\")\n",
    "\n",
    "print(\"Tokenizer training complete! Saved to\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 437213\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load your trained tokenizer\n",
    "tokenizer = BertTokenizer(vocab_file=\"custom_vocab-vocab.txt\", do_lower_case=False)\n",
    "\n",
    "# File containing books (one per line)\n",
    "file_path = \"../out/cleaned_books.txt\"\n",
    "\n",
    "# Parameters for sliding window\n",
    "block_size = 128  # Max sequence length\n",
    "stride = 64  # Overlap between chunks (adjust as needed)\n",
    "\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size, stride):\n",
    "        self.examples = []\n",
    "\n",
    "        # Read the file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    tokens = tokenizer.encode(line.strip(), add_special_tokens=True)\n",
    "\n",
    "                    # Process the line using a sliding window\n",
    "                    for i in range(0, len(tokens) - block_size + 1, stride):\n",
    "                        chunk = tokens[i : i + block_size]  # Take a block of size 128\n",
    "                        self.examples.append(chunk)\n",
    "\n",
    "                    # If the last chunk is shorter than block_size, include it\n",
    "                    if len(tokens) > block_size and len(tokens) % stride != 0:\n",
    "                        self.examples.append(tokens[-block_size:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\"input_ids\": self.examples[i]}\n",
    "\n",
    "\n",
    "# Create dataset using sliding window\n",
    "dataset = SlidingWindowDataset(file_path, tokenizer, block_size, stride)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Total sequences: {len(dataset)}\")  # More sequences now!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample (token IDs): {'input_ids': [45390, 5853, 18, 384, 1117, 300, 290, 3721, 877, 317, 302, 13828, 18, 2256, 290, 3721, 921, 302, 8053, 865, 4227, 18, 48353, 354, 1624, 18, 731, 3618, 302, 3997, 18, 48353, 2147, 422, 445, 427, 319, 728, 2304, 18, 430, 3997, 627, 302, 1111, 18, 49239, 2676, 18, 18, 18, 3907, 921, 48353, 610, 5853, 18, 17, 371, 18, 18, 18, 336, 562, 8385, 16, 908, 301, 1111, 18, 371, 18, 18, 18, 336, 374, 388, 308, 290, 3721, 8825, 18, 18, 18, 477, 18, 18, 18, 862, 562, 336, 35, 17, 583, 948, 317, 290, 5133, 2035, 300, 4438, 4230, 16, 908, 48353, 18, 17, 801, 302, 48069, 35, 8385, 2343, 308, 290, 2805, 18, 17, 2008, 546, 5, 5002, 537, 489, 1239, 8284, 334, 5]}\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# Access the first item\n",
    "example = dataset[1]  \n",
    "\n",
    "# Print the tokenized output\n",
    "print(\"First sample (token IDs):\", example)\n",
    "print(len(example[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: sluipt dichterbij. De deur van de kast staat op een kier. Uit de kast komt een schoentje tevoorschijn. Lila is bang. Er volgt een been. Lila vraagt zich af wat dat alles betekent. Het been wordt een kind. Ooooh... zachtjes komt Lila iets dichterbij. - Ik... ik ben Tom, zegt het kind. Ik... ik had me in de kast verstopt... En... Waar ben ik? - Je bent op de stortplaats van Merlijn, zegt Lila. - Op een vuilnisbelt? Tom kijkt in de verte. - Nou zeg! Mama zal wel erg ongerust zijn!\n"
     ]
    }
   ],
   "source": [
    "# Decode the first example back to text\n",
    "decoded_text = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Using a Custom Tokenizer\n",
    "\n",
    "I save my trained tokenizer in **Hugging Face format**, which makes it reusable with models from the `transformers` library. In the second block, I use this tokenizer to process my dataset into **tokenized sequences of length 128**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer successfully saved in Hugging Face format!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer with only the vocab file\n",
    "hf_tokenizer = BertTokenizerFast(vocab_file=vocab_file, do_lower_case=False)\n",
    "\n",
    "# Add special tokens to tokenizer\n",
    "hf_tokenizer.add_special_tokens({\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"mask_token\": \"[MASK]\"\n",
    "})\n",
    "\n",
    "# Save tokenizer in Hugging Face format\n",
    "hf_tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Tokenizer successfully saved in Hugging Face format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: {'train': Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 3616\n",
      "}), 'validation': Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 452\n",
      "}), 'test': Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 452\n",
      "})}\n",
      "Tokenization complete!\n",
      "{'input_ids': tensor([  13, 4663,   14,  223,  338,  182, 3617,  311,  175, 2967,   14,    8,\n",
      "           8,  427,   28,    8,    8,  463,  492,  306,  205,  182, 3617,  232,\n",
      "          14,   14,   14,    8,    8,  527,   28,    8,    8, 1179,   12,  205,\n",
      "         338,  182, 3617,  311,  175, 2967,   14,    8,    8,   43,   14, 1313,\n",
      "        1720,  208,  659, 1787,   28,    8,    8,  797,   14,   14,   14,  208,\n",
      "         201,  576,   14,   14,   14,  576,  182,  387,  183,  231,   14,    8,\n",
      "           8, 1179,   12,  208,  734,  252,  359,  358,  348,   12,    8,  283,\n",
      "         191, 2274,   14,    8,  527,   14,   14,   14,    8,  257, 2721,  216,\n",
      "        1675,  348,  198,  216, 4015,  177,  205,  380, 1873,  182,  580,  391,\n",
      "         939,  610,  194,  302, 4833,  177, 2025,  227,  302, 1782,  724,  585,\n",
      "          14,  223, 3896,  184,  198,   12, 1099,  184])}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset as a single string\n",
    "with open(\"../out/cleaned_books_small.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"custom_tokenizer\")\n",
    "\n",
    "# Tokenize entire text\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]  # Flatten tensor\n",
    "\n",
    "# Split into chunks of 128 tokens\n",
    "chunk_size = 128\n",
    "chunks = [tokenized_text[i : i + chunk_size] for i in range(0, len(tokenized_text), chunk_size)]\n",
    "\n",
    "# Convert to dataset format\n",
    "dataset = Dataset.from_dict({\"input_ids\": chunks})\n",
    "\n",
    "# Split into train (80%), val (10%), and test (10%)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "valid_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Final dataset splits\n",
    "final_datasets = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": valid_test_split[\"train\"],\n",
    "    \"test\": valid_test_split[\"test\"],\n",
    "}\n",
    "\n",
    "print(\"Dataset:\", final_datasets)\n",
    "\n",
    "# Convert to PyTorch format\n",
    "for split in final_datasets:\n",
    "    final_datasets[split].set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "\n",
    "# Print the first item in the test set\n",
    "print(final_datasets[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(5000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(128, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=5000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a new BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072, \n",
    "    max_position_embeddings=128, \n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.cls_token_id,\n",
    "    eos_token_id=tokenizer.sep_token_id\n",
    ")\n",
    "\n",
    "# Initialize a new BERT model with this config\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "# Adjust the model's vocabulary size to match the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly mask words in MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/bp07d_fj08bd4h_x3m2q3bcr0000gp/T/ipykernel_1536/1607018213.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../bert_custom_checkpoints\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=226,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=226, \n",
    "    num_train_epochs=4,\n",
    "    save_total_limit=None,\n",
    "    overwrite_output_dir=False,\n",
    "    logging_dir=\"../logs\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../bert_custom_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1461\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1461\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1476\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1118\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[0;32m-> 1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_decoder\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     encoder_batch_size, encoder_sequence_length, _ \u001b[38;5;241m=\u001b[39m encoder_hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m   1120\u001b[0m     encoder_hidden_shape \u001b[38;5;241m=\u001b[39m (encoder_batch_size, encoder_sequence_length)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/configuration_utils.py:208\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    205\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(key, value)\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    210\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"../bert_custom_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the results of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /Users/jonasklein/bert_custom_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/Users/jonasklein/bert_custom_final\"\n",
    "tokenizer_path = \"/Users/jonasklein/custom_tokenizer\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/bp07d_fj08bd4h_x3m2q3bcr0000gp/T/ipykernel_1536/378140489.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 04:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.088505268096924,\n",
       " 'eval_model_preparation_time': 0.0011,\n",
       " 'eval_runtime': 7.6665,\n",
       " 'eval_samples_per_second': 58.958,\n",
       " 'eval_steps_per_second': 7.435}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=final_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 458.1433\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "test_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "perplexity = math.exp(test_loss)\n",
    "\n",
    "print(f\"Test Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract embedding for a certain word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'laag':\n",
      "torch.Size([1, 1, 768])\n",
      "Token Embedding Shape: torch.Size([768])\n",
      "Token Embedding: tensor([ 2.9236,  1.0079, -0.8346, -0.0273, -1.1866, -0.6646,  0.2908,  0.0036,\n",
      "         1.3928, -0.6854, -0.1433,  1.7425, -1.7796,  0.0531,  0.0398, -0.6935,\n",
      "        -0.5568, -0.5510,  0.5848, -0.2226,  1.6323, -0.3731,  1.0677, -1.6748,\n",
      "         0.0895, -1.0691, -1.7554,  0.2883, -0.1404,  1.4276,  0.5130, -1.4716,\n",
      "        -1.4637, -0.8977,  0.8635,  0.0059,  1.6054, -0.5841,  0.3528,  1.0565,\n",
      "         2.3417, -1.2483, -1.0305, -1.0594,  0.3393,  0.3274, -0.8450, -0.0059,\n",
      "        -0.1178,  1.7839, -0.0681, -0.7162, -1.1627,  0.0797, -0.7405,  1.3743,\n",
      "        -0.4501,  0.0983,  0.3862, -0.8646, -0.8856, -0.1382, -0.9937, -1.0002,\n",
      "         0.2911, -1.2921,  0.8013, -1.0002,  0.3577,  0.2958,  0.7073, -0.9498,\n",
      "        -0.0648,  1.6411,  0.2564, -0.1371,  0.7749,  0.5836,  0.5437,  0.9387,\n",
      "        -0.8600,  0.0299,  2.8112, -1.4452,  0.8423, -0.0889,  0.7509, -2.0065,\n",
      "         0.0670, -0.6566, -1.2987,  1.9810, -0.4532,  0.1975, -0.4573,  1.5943,\n",
      "         0.1950,  0.4528,  0.2496, -0.1178,  0.4209,  0.0680,  0.2979,  0.9745,\n",
      "        -0.5592,  0.2103, -0.8830,  0.7223,  0.6490, -1.3668, -1.0508,  1.5080,\n",
      "         1.3805,  0.4414, -2.2751,  0.1209, -0.7068,  0.1868, -0.1078,  1.0963,\n",
      "        -0.1645, -0.4924, -0.1823,  1.5514, -1.3473, -1.2487, -1.1151,  0.5372,\n",
      "        -1.0782,  0.0114,  0.5056, -1.7062,  0.4189, -0.7073,  2.0336,  0.4578,\n",
      "         0.7255, -1.4355, -0.2795,  0.5890, -0.1174, -0.2582, -0.6053, -1.5284,\n",
      "         1.1691,  0.3313, -1.2190, -0.7210, -1.2009,  0.1289, -0.4410,  0.5755,\n",
      "         0.1907, -0.6073,  0.6530,  0.2892, -0.3382, -1.4558, -0.5167,  0.9763,\n",
      "        -0.1228,  0.5450,  0.9336, -0.7137, -1.0364, -1.4796,  1.7968, -0.5918,\n",
      "         0.9221, -0.1402,  2.0514, -0.5599,  0.2032, -2.0218,  0.0215,  1.1970,\n",
      "         1.6208,  1.5244, -0.3666,  0.1449,  2.2187, -0.3577, -0.0858,  1.5246,\n",
      "        -2.4270,  0.4679, -2.8125, -0.8964, -1.5562,  1.1801, -1.0445, -1.0617,\n",
      "         1.5555, -0.2009,  0.9559,  0.3441,  0.0225, -1.0290, -1.2992,  0.3909,\n",
      "        -2.1845,  1.7687, -1.2449,  0.0179, -0.7570,  1.1896, -0.8254, -1.5252,\n",
      "         0.9764,  1.1906,  1.9066, -0.5073,  0.6956,  0.4560,  0.0562,  0.8564,\n",
      "        -0.0314,  0.3673, -0.5383, -1.1061,  0.7987,  0.4649,  0.6700,  0.7322,\n",
      "        -1.4880, -0.7691,  0.1645, -0.8869, -0.5684, -0.9160, -0.6497,  0.3128,\n",
      "        -1.1177, -0.0617, -0.4673,  0.8822, -0.1310,  0.1717, -1.5098, -0.2336,\n",
      "        -0.1330,  0.1103, -1.4912, -0.7336, -0.0293,  0.9209,  1.2685,  0.9083,\n",
      "        -0.4717, -1.0102,  1.1152,  0.4416,  0.3093,  1.5304,  1.0819,  0.6616,\n",
      "        -1.4255,  0.0702, -0.4490, -0.4871, -1.2271,  0.1191,  0.6097, -0.2478,\n",
      "         0.0419, -0.2268, -1.4699, -1.0423,  0.8129, -0.2736, -0.1369,  0.7883,\n",
      "        -2.0272,  0.2029,  0.1899, -1.4595, -0.7075, -2.1277, -0.3833, -0.0830,\n",
      "         0.2570, -0.7934,  0.0324,  0.5608, -2.0063,  0.0075, -0.9843, -1.6952,\n",
      "         2.2350, -0.4142,  1.6858, -0.2636, -1.3999,  1.1857,  0.6892, -1.1527,\n",
      "        -0.5839,  0.7715, -0.2903,  1.9282, -0.5156,  1.6316,  0.2925,  2.8651,\n",
      "        -0.2201, -0.2795, -0.0173, -0.3569, -1.1693, -0.4092, -1.5557,  1.4741,\n",
      "         0.4403, -1.1342,  1.6304,  1.3693,  0.1287,  1.0950, -0.6310, -0.5229,\n",
      "         0.4196,  0.0543, -0.5382,  0.3904, -0.2177,  0.9411, -0.7484, -0.8207,\n",
      "         0.1267, -1.1940,  0.3239,  1.1154, -0.7514,  0.1054, -0.9843, -0.7636,\n",
      "         0.7836,  0.5205, -1.0498,  0.1825, -1.7440,  0.5061,  0.3465, -0.6188,\n",
      "         0.1693, -0.5466,  2.1172,  0.4681,  0.7304, -1.5385,  0.8358,  0.0890,\n",
      "         0.2030, -1.2587,  0.9045,  1.4040, -0.1006, -1.4003,  1.2925, -1.9771,\n",
      "        -0.5053, -2.7437, -0.8334,  0.8481, -0.4898,  0.9775,  0.3232, -0.5016,\n",
      "         0.2914,  0.9659,  0.7881, -0.7795,  2.0635, -0.6866,  0.0685, -0.4462,\n",
      "        -2.1213, -0.5079,  0.0380, -1.0676,  0.9153,  0.7463, -0.1131, -1.0212,\n",
      "        -1.7226, -0.2861,  0.7971,  0.4192, -0.9757,  0.0626, -0.6298,  0.2221,\n",
      "         1.4806, -0.6930,  0.4199,  2.5317, -0.5597, -2.4252, -0.4465,  0.1287,\n",
      "         0.6176, -1.0633, -0.7632, -1.1446, -0.3264, -0.4927, -1.1188,  0.1719,\n",
      "         0.9766, -0.0509,  0.0225,  0.7027,  0.0683,  0.2358, -0.7207,  0.4726,\n",
      "         0.0440, -1.2134,  0.7134,  0.3756, -0.3337, -0.1240, -1.9141,  1.5526,\n",
      "         0.6248, -0.0036, -1.3127, -1.0082,  0.4884,  1.6713,  0.0043, -0.1197,\n",
      "        -0.2359,  0.4805,  0.2384,  0.9726,  0.4079, -1.2057, -0.6590,  1.6878,\n",
      "        -0.3690,  0.3552,  0.0798,  0.3372, -0.0294,  0.8902, -0.4654, -2.3526,\n",
      "         0.3907, -1.1240,  0.2465, -0.3194,  1.0925, -1.2132,  0.6085, -0.6039,\n",
      "         2.0151, -1.0508,  0.4326, -0.1014,  1.5593,  0.2643,  0.7661, -0.2499,\n",
      "         0.7170,  0.6805,  0.8543, -1.1564,  1.0223, -0.2175,  0.8838,  2.4094,\n",
      "        -2.0812,  0.0959,  0.4735,  0.1785, -1.1820,  1.0367, -1.8988, -0.5178,\n",
      "        -0.2946, -0.5225,  1.8677, -1.3719, -1.0058, -0.2512,  0.2765, -0.7801,\n",
      "        -0.4826, -0.1109, -2.9328, -1.3022,  1.4733,  1.3060,  0.4844,  1.5262,\n",
      "        -1.7111,  1.6731,  0.4150, -0.8570,  1.0663,  0.1514,  1.2225, -0.2270,\n",
      "         1.5413,  0.5554, -0.1414,  0.5553,  1.5335, -0.4975,  0.6290, -0.0209,\n",
      "        -0.3913,  1.5108,  2.7028, -2.3299,  0.5642,  0.4710, -0.7987, -0.0637,\n",
      "         0.5783,  1.2149, -0.3881, -1.2721,  0.7873,  0.2179, -0.1308, -0.0898,\n",
      "        -1.5177,  0.4266, -0.0980, -0.9231, -0.9972, -0.3587,  0.8759,  0.6021,\n",
      "        -0.1524, -1.2949, -0.0296, -1.3082,  1.9671,  0.7280, -1.0188,  0.9710,\n",
      "         1.0618, -0.5251,  0.7164,  1.1844, -0.9746,  1.0868,  0.0414,  1.1853,\n",
      "         0.8099, -0.7839,  0.8537, -0.4184,  0.1680,  1.8148,  2.9763,  0.3281,\n",
      "        -0.7860, -1.6774,  1.2898,  0.3950, -0.1812, -1.6168,  0.1921, -1.0398,\n",
      "         1.5397, -0.1756,  0.6878,  0.7267, -0.5902, -0.3357, -0.3925,  0.8516,\n",
      "        -0.1793,  1.3989, -1.1232, -1.6100,  0.6983,  0.7917,  1.0523,  0.2765,\n",
      "        -1.7416,  1.2478,  0.6380, -0.8867, -0.3165, -0.5379, -0.7848,  0.0510,\n",
      "        -1.0303,  0.3888, -0.9473, -0.4740, -2.1371, -1.1229, -0.0477, -0.9037,\n",
      "        -0.1665, -1.7151,  1.2655,  1.2330, -2.5908, -0.1602,  0.3738, -0.8611,\n",
      "        -1.0396,  0.3722, -0.0286,  0.7864,  0.6617,  1.4321, -0.3466, -0.9765,\n",
      "         1.2998, -0.8583,  0.3478,  0.8533,  1.2683,  0.0741,  0.3640,  2.2018,\n",
      "         0.3795, -0.5222,  1.6035,  0.1827,  0.7339, -1.4662,  0.8660, -0.3302,\n",
      "         1.2222,  0.1546,  0.4350, -0.4317, -0.0299, -0.0764, -1.3850,  0.2188,\n",
      "        -0.6634, -0.1275,  0.2283,  0.9308,  0.8248, -0.0031,  1.8831,  1.1575,\n",
      "         0.2364, -0.0455,  0.7898,  0.0368,  0.9014,  0.0829,  0.2610,  0.8177,\n",
      "        -2.0368, -0.0332, -0.6192,  0.3834, -0.0781,  0.5784,  0.5665, -0.3055,\n",
      "        -0.4247,  0.5956,  0.3802,  1.2330, -1.3552,  0.1080,  1.5331,  0.5540,\n",
      "         1.8718, -0.3571, -0.8127,  1.9562,  0.4900, -1.3708,  1.0764,  0.3643,\n",
      "        -0.7536,  0.3606,  0.7470,  0.1135,  1.9705, -0.5126,  1.1334, -0.9430,\n",
      "        -0.2660,  0.3831,  0.8455, -0.1706, -0.1663, -2.1063, -1.2247,  0.7864,\n",
      "         0.1574,  0.9271,  0.3119,  1.4267,  0.2535, -1.5836, -0.3047, -0.1253,\n",
      "        -1.5824,  0.4435, -0.5774, -0.5615,  0.4701, -1.4461, -0.7645,  1.1884,\n",
      "        -0.8924,  0.2031, -0.9125,  0.3627, -0.1337, -1.2519,  0.9325,  0.6922,\n",
      "        -0.3254,  2.2417, -0.9336,  1.2368,  0.8257, -0.8797,  0.4987, -0.8256,\n",
      "         0.6444,  1.8627, -0.4789, -1.4940,  0.7956,  0.2514, -0.9914,  0.3903,\n",
      "         0.9163, -0.3192, -0.4210,  0.4047, -0.1371, -0.8747,  1.0210,  0.4226,\n",
      "         0.6290, -0.6217,  0.5201,  0.3064,  0.2906, -0.3106, -1.4282,  1.5226,\n",
      "         1.5341,  0.3342, -0.1591, -0.6355, -0.9374,  0.6806, -0.0379, -0.6494,\n",
      "        -0.6389,  0.6534, -0.9529, -0.0231, -2.2580, -0.6716, -0.4532, -1.8188])\n"
     ]
    }
   ],
   "source": [
    "token = \"laag\"\n",
    "\n",
    "# Convert token to token ID\n",
    "token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "# Check if token is in the vocabulary\n",
    "if token_id is None or token_id == tokenizer.unk_token_id:\n",
    "    print(f\"Token '{token}' not found in the vocabulary! The embedding will be the one for the [UNK] token.\")\n",
    "else:\n",
    "    print(f\"Embedding for '{token}':\")\n",
    "\n",
    "# Convert token ID to tensor\n",
    "input_ids = torch.tensor([[token_id]])  # Shape: (1, 1) -> Batch of 1, 1 token\n",
    "\n",
    "# Get the model's input embeddings\n",
    "#with torch.no_grad():\n",
    "#    embedding_layer = model.get_input_embeddings()  # Extract input embeddings\n",
    "#    token_embedding = embedding_layer(input_ids)  # Get embedding vector\n",
    "\n",
    "# Get the output from the model (excluding MLM head, so the last hidden layer)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, output_hidden_states=True) # This makes sure it is only passed through the BERT encoder\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "# Extract the last hidden layer\n",
    "last_hidden_state = hidden_states[-1]\n",
    "\n",
    "# Retrieve the token embedding\n",
    "print(last_hidden_state.shape)\n",
    "token_embedding = last_hidden_state[0, -1, :]\n",
    "\n",
    "print(\"Token Embedding Shape:\", token_embedding.shape) # Shape: (768,)\n",
    "print(\"Token Embedding:\", token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2358\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(\"laag\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute cosine similarity between embeddings of two different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "Cosine Similarity between 'laag' and 'hoog': 0.5117028951644897\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2, dim=-1)\n",
    "\n",
    "token1 = \"laag\"\n",
    "token2 = \"hoog\"\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_id1 = tokenizer.convert_tokens_to_ids(token1)\n",
    "token_id2 = tokenizer.convert_tokens_to_ids(token2)\n",
    "\n",
    "# Ensure both tokens exist in the vocabulary\n",
    "if token_id1 is None or token_id2 is None:\n",
    "    raise ValueError(f\"One of the tokens ('{token1}', '{token2}') is not in the vocabulary.\")\n",
    "\n",
    "# Convert token IDs to tensors\n",
    "input_ids1 = torch.tensor([[token_id1]])\n",
    "input_ids2 = torch.tensor([[token_id2]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids1, output_hidden_states=True) \n",
    "    hidden_states1 = outputs.hidden_states\n",
    "\n",
    "last_hidden_state1 = hidden_states1[-1]\n",
    "\n",
    "# Retrieve the token embedding\n",
    "print(last_hidden_state1.shape)\n",
    "token_embedding1 = last_hidden_state1[0, -1, :]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids2, output_hidden_states=True)\n",
    "    hidden_states2 = outputs.hidden_states\n",
    "\n",
    "# Extract the last hidden layer\n",
    "last_hidden_state2 = hidden_states2[-1]\n",
    "\n",
    "# Retrieve the token embedding\n",
    "print(last_hidden_state2.shape)\n",
    "token_embedding2 = last_hidden_state2[0, -1, :]\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = cosine_similarity(token_embedding1, token_embedding2)\n",
    "print(f\"Cosine Similarity between '{token1}' and '{token2}':\", similarity.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
